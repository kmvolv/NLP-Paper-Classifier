{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow==1.13.2 (from versions: 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.19.0rc0)\n",
      "ERROR: No matching distribution found for tensorflow==1.13.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow==1.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp39-cp39-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.18.0-cp39-cp39-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.8.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.2)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading grpcio-1.70.0-cp39-cp39-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading keras-3.9.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.3)\n",
      "Collecting h5py>=3.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading h5py-3.13.0-cp39-cp39-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp39-cp39-win_amd64.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading optree-0.14.1-cp39-cp39-win_amd64.whl.metadata (50 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2022.12.7)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (8.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.21.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.18.0-cp39-cp39-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.18.0-cp39-cp39-win_amd64.whl (390.0 MB)\n",
      "   ---------------------------------------- 0.0/390.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.9/390.0 MB 23.4 MB/s eta 0:00:17\n",
      "    --------------------------------------- 7.3/390.0 MB 23.8 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 11.5/390.0 MB 20.0 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 17.0/390.0 MB 21.5 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 23.1/390.0 MB 22.4 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 25.2/390.0 MB 20.4 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 28.3/390.0 MB 20.9 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 33.6/390.0 MB 20.3 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 36.7/390.0 MB 19.8 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 41.4/390.0 MB 20.1 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 46.1/390.0 MB 20.5 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 48.5/390.0 MB 19.7 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 54.5/390.0 MB 20.5 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 60.6/390.0 MB 21.0 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 62.4/390.0 MB 20.1 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 68.2/390.0 MB 20.5 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 73.1/390.0 MB 20.7 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 78.6/390.0 MB 21.3 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 82.6/390.0 MB 21.0 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 88.1/390.0 MB 21.2 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 94.4/390.0 MB 21.7 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 99.9/390.0 MB 21.9 MB/s eta 0:00:14\n",
      "   ---------- ---------------------------- 107.0/390.0 MB 22.4 MB/s eta 0:00:13\n",
      "   ----------- --------------------------- 112.7/390.0 MB 22.6 MB/s eta 0:00:13\n",
      "   ----------- --------------------------- 118.5/390.0 MB 22.9 MB/s eta 0:00:12\n",
      "   ------------ -------------------------- 124.0/390.0 MB 23.0 MB/s eta 0:00:12\n",
      "   ------------- ------------------------- 131.1/390.0 MB 23.3 MB/s eta 0:00:12\n",
      "   ------------- ------------------------- 136.3/390.0 MB 23.5 MB/s eta 0:00:11\n",
      "   -------------- ------------------------ 142.6/390.0 MB 23.7 MB/s eta 0:00:11\n",
      "   -------------- ------------------------ 146.0/390.0 MB 23.4 MB/s eta 0:00:11\n",
      "   -------------- ------------------------ 149.9/390.0 MB 23.3 MB/s eta 0:00:11\n",
      "   --------------- ----------------------- 155.5/390.0 MB 23.3 MB/s eta 0:00:11\n",
      "   ---------------- ---------------------- 161.7/390.0 MB 23.5 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 167.8/390.0 MB 23.7 MB/s eta 0:00:10\n",
      "   ----------------- --------------------- 174.1/390.0 MB 23.9 MB/s eta 0:00:10\n",
      "   ----------------- --------------------- 179.3/390.0 MB 23.9 MB/s eta 0:00:09\n",
      "   ------------------ -------------------- 185.9/390.0 MB 24.1 MB/s eta 0:00:09\n",
      "   ------------------ -------------------- 189.8/390.0 MB 24.0 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 196.6/390.0 MB 24.2 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 203.4/390.0 MB 24.3 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 209.7/390.0 MB 24.5 MB/s eta 0:00:08\n",
      "   --------------------- ----------------- 213.9/390.0 MB 24.5 MB/s eta 0:00:08\n",
      "   --------------------- ----------------- 219.9/390.0 MB 24.5 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 221.5/390.0 MB 24.1 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 227.5/390.0 MB 24.3 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 234.9/390.0 MB 24.5 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 239.1/390.0 MB 24.5 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 243.3/390.0 MB 24.3 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 247.5/390.0 MB 24.4 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 250.3/390.0 MB 24.0 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 257.2/390.0 MB 24.2 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 263.7/390.0 MB 24.4 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 269.5/390.0 MB 24.7 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 273.4/390.0 MB 24.5 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 276.8/390.0 MB 24.4 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 283.1/390.0 MB 24.5 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 289.4/390.0 MB 24.8 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 295.7/390.0 MB 25.2 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 300.7/390.0 MB 25.2 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 307.0/390.0 MB 25.4 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 312.5/390.0 MB 25.8 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 315.6/390.0 MB 25.5 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 321.1/390.0 MB 25.5 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 328.2/390.0 MB 25.9 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 330.6/390.0 MB 25.7 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 336.9/390.0 MB 25.7 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 341.8/390.0 MB 26.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 347.1/390.0 MB 25.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 353.4/390.0 MB 25.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 359.7/390.0 MB 26.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 364.9/390.0 MB 26.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 370.7/390.0 MB 25.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 376.4/390.0 MB 25.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  382.7/390.0 MB 25.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  389.5/390.0 MB 25.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  389.8/390.0 MB 25.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  389.8/390.0 MB 25.9 MB/s eta 0:00:01\n",
      "   --------------------------------------- 390.0/390.0 MB 24.5 MB/s eta 0:00:00\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.70.0-cp39-cp39-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.3/4.3 MB 25.9 MB/s eta 0:00:00\n",
      "Downloading h5py-3.13.0-cp39-cp39-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.0/3.0 MB 28.7 MB/s eta 0:00:00\n",
      "Downloading keras-3.9.0-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 23.0 MB/s eta 0:00:00\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Downloading ml_dtypes-0.4.1-cp39-cp39-win_amd64.whl (126 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 26.0 MB/s eta 0:00:00\n",
      "Using cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.1-cp39-cp39-win_amd64.whl (291 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, opt-einsum, ml-dtypes, mdurl, h5py, grpcio, google-pasta, gast, astunparse, absl-py, markdown-it-py, markdown, tensorboard, rich, keras, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.70.0 h5py-3.13.0 keras-3.9.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.14.1 rich-13.9.4 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-intel-2.18.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.5.0 werkzeug-3.1.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "from tqdm.notebook import tqdm\n",
    "import re, os\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'text'\n",
    "LABEL_COLUMN = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"QTL_text.csv\")\n",
    "\n",
    "# rename category column as label\n",
    "df.rename(columns={'Category': 'label'}, inplace=True) \n",
    "\n",
    "# combine title and abstract into one column called text\n",
    "df['text'] = df['Title'] + ' ' + df['Abstract']\n",
    "\n",
    "\n",
    "# only keep label and text columns\n",
    "df = df[['label', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rohai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra whitespace\n",
    "    text = \" \".join([stemmer.stem(word) for word in text.split() if word not in stop_words])  # Remove stopwords and stem\n",
    "    return text\n",
    "\n",
    "\n",
    "df[\"Cleaned Text\"] = df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "train_ind, test_ind = train_test_split(df.index, test_size=test_ratio, shuffle=True, random_state=3)\n",
    "\n",
    "train_df = df.loc[train_ind,:]\n",
    "test_df = df.loc[test_ind,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random oversampling of minority class (label 1)\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_resample(train_df[[\"Cleaned Text\"]], train_df[\"label\"])\n",
    "\n",
    "train_df = pd.DataFrame(X_resampled, columns=[\"Cleaned Text\"])\n",
    "train_df[\"label\"] = y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling the majority class (label 0) to balance the dataset\n",
    "\n",
    "min_train_count = train_df[\"label\"].value_counts().min()\n",
    "train_df = pd.concat([group.sample(min_train_count) for name, group in train_df.groupby(\"label\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df[[\"Cleaned Text\", \"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=3)\n",
    "\n",
    "# Access train and test splits\n",
    "train_texts = split_dataset['train']\n",
    "test_texts = split_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614cedd0a3f3459c9ac53a134b81bec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9022 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2e1206b7b347fe90180da1ef102815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"Cleaned Text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = train_texts.map(tokenize_function, batched=True)\n",
    "test_dataset = test_texts.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_df[\"Cleaned Text\"].values\n",
    "train_labels = train_df[\"label\"].values\n",
    "\n",
    "val_text = test_df[\"Cleaned Text\"].values\n",
    "val_labels = test_df[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2, output_attentions=False, output_hidden_states=True)\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_token_ids = []\n",
    "train_attention_masks = []\n",
    "for description in train_text:\n",
    "  encoding_dict = tokenizer.encode_plus(\n",
    "    description,\n",
    "      add_special_tokens=True,\n",
    "      max_length=512,\n",
    "      padding='max_length',  \n",
    "      truncation=True,       \n",
    "      return_attention_mask=True,\n",
    "      return_tensors=\"pt\"\n",
    "  )\n",
    "  train_token_ids.append(encoding_dict[\"input_ids\"]) \n",
    "  train_attention_masks.append(encoding_dict[\"attention_mask\"])\n",
    "\n",
    "train_token_ids = torch.cat(train_token_ids, dim=0)\n",
    "train_attention_masks = torch.cat(train_attention_masks, dim=0)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "val_token_ids = []\n",
    "val_attention_masks = []\n",
    "for description in val_text:\n",
    "  encoding_dict = tokenizer.encode_plus(\n",
    "    description,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    padding='max_length',  \n",
    "    truncation=True,       \n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\"\n",
    "  )\n",
    "  val_token_ids.append(encoding_dict[\"input_ids\"]) \n",
    "  val_attention_masks.append(encoding_dict[\"attention_mask\"])\n",
    "\n",
    "val_token_ids = torch.cat(val_token_ids, dim=0)\n",
    "val_attention_masks = torch.cat(val_attention_masks, dim=0)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "train_set = TensorDataset(train_token_ids, train_attention_masks, train_labels)\n",
    "val_set = TensorDataset(val_token_ids, val_attention_masks, val_labels)\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataloader = DataLoader(train_set, sampler=RandomSampler(train_set), batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_set, sampler=SequentialSampler(val_set), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(predictions, labels):\n",
    "    preds_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "epochs = 12\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(val_dataloader):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in val_dataloader:\n",
    "        \n",
    "        batch = tuple(b.to(\"cuda\") for b in batch)\n",
    "        \n",
    "        inputs = {\"input_ids\":      batch[0],\n",
    "                  \"attention_mask\": batch[1],\n",
    "                  \"labels\":         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs[\"labels\"].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(val_dataloader) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ab7d8bf64b40ba8a9113616a983da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008dc8215712462c86508ea04114a4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 4.00 GiB total capacity; 10.61 GiB already allocated; 0 bytes free; 10.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 19\u001b[0m\n\u001b[0;32m     12\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(b\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch)\n\u001b[0;32m     14\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m:      batch[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     15\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m     16\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m:         batch[\u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m     17\u001b[0m          }       \n\u001b[1;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     22\u001b[0m loss_train_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1673\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1665\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1666\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1667\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1669\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1671\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1673\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1683\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1685\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1687\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:304\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    301\u001b[0m     past_key_value \u001b[38;5;241m=\u001b[39m (key_layer, value_layer)\n\u001b[0;32m    303\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key_query\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    307\u001b[0m     query_length, key_length \u001b[38;5;241m=\u001b[39m query_layer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], key_layer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 4.00 GiB total capacity; 10.61 GiB already allocated; 0 bytes free; 10.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(train_dataloader, desc=\"Epoch {:1d}\".format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(\"cuda\") for b in batch)\n",
    "        \n",
    "        inputs = {\"input_ids\":      batch[0],\n",
    "                  \"attention_mask\": batch[1],\n",
    "                  \"labels\":         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({\"training_loss\": \"{:.3f}\".format(loss.item()/len(batch))})\n",
    "         \n",
    "    # save model to directory, if it doesn't exist create it\n",
    "    if not os.path.exists(\"./content/\"):\n",
    "        os.makedirs(\"./content/\")\n",
    "   \n",
    "    torch.save(model.state_dict(), f\"./bertfiles/finetuned_BERT_epoch_{epoch}.model\")\n",
    "        \n",
    "    tqdm.write(f\"\\nEpoch {epoch}\")\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(train_dataloader)            \n",
    "    tqdm.write(f\"Training loss: {loss_train_avg}\")\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(val_dataloader)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f\"Validation loss: {val_loss}\")\n",
    "    tqdm.write(f\"F1 Score (Weighted): {val_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels=2, \n",
    "    state_dict=torch.load(\"./content/finetuned_BERT_epoch_5.model\", map_location=\"cuda\")\n",
    ")\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clean_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m check_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_unlabeled.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m check_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m check_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m check_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbstract\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m check_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaned Text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m check_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[43mclean_text\u001b[49m)\n\u001b[0;32m      6\u001b[0m check_text \u001b[38;5;241m=\u001b[39m check_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaned Text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      8\u001b[0m check_token_ids \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_text' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Starting preprocessing...\")\n",
    "check_df = pd.read_csv(\"test_unlabeled.csv\")\n",
    "check_df[\"text\"] = check_df[\"Title\"] + \" \" + check_df[\"Abstract\"]\n",
    "check_df[\"Cleaned Text\"] = check_df[\"text\"].apply(clean_text)\n",
    "\n",
    "check_text = check_df[\"Cleaned Text\"].values\n",
    "\n",
    "check_token_ids = []\n",
    "check_attention_masks = []\n",
    "for description in check_text:\n",
    "  encoding_dict = tokenizer.encode_plus(description, add_special_tokens=True, max_length=256 ,  pad_to_max_length=True, return_attention_mask=True, return_tensors=\"pt\")\n",
    "  check_token_ids.append(encoding_dict[\"input_ids\"]) \n",
    "  check_attention_masks.append(encoding_dict[\"attention_mask\"])\n",
    "\n",
    "check_token_ids = torch.cat(check_token_ids, dim=0)\n",
    "check_attention_masks = torch.cat(check_attention_masks, dim=0)\n",
    "\n",
    "check_set = TensorDataset(check_token_ids, check_attention_masks)\n",
    "check_dataloader = DataLoader(check_set, sampler=SequentialSampler(check_set), batch_size=batch_size)\n",
    "\n",
    "print(\"Finished preprocessing, now loading model...\")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels=2, \n",
    "    state_dict=torch.load(\"./bertfiles/checkpoints/finetuned_BERT_epoch_12.model\", map_location=\"cuda\")\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded, now predicting...\")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# No gradient calculation is needed during inference\n",
    "with torch.no_grad():\n",
    "    progress = tqdm(check_dataloader, desc=\"Predicting\")\n",
    "    for batch in progress:\n",
    "        batch = tuple(b.to(\"cuda\") for b in batch)\n",
    "\n",
    "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1]}\n",
    "\n",
    "        # Get model outputs\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # Extract logits\n",
    "\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        # Convert logits to predicted class\n",
    "        preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "print(\"Prediction complete!\")\n",
    "\n",
    "# predictions = np.concatenate(predictions, axis=0)\n",
    "# predictions = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "check_df[\"Category\"] = predictions\n",
    "check_df.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'LABEL_1', 'score': 0.5241997, 'index': 1, 'word': 'In', 'start': 0, 'end': 2}, {'entity': 'LABEL_0', 'score': 0.5435446, 'index': 2, 'word': 'a', 'start': 3, 'end': 4}, {'entity': 'LABEL_1', 'score': 0.5095409, 'index': 3, 'word': 'previous', 'start': 5, 'end': 13}, {'entity': 'LABEL_0', 'score': 0.5702159, 'index': 4, 'word': 'study', 'start': 14, 'end': 19}, {'entity': 'LABEL_0', 'score': 0.5313562, 'index': 5, 'word': ',', 'start': 19, 'end': 20}, {'entity': 'LABEL_0', 'score': 0.5712199, 'index': 6, 'word': 'Q', 'start': 21, 'end': 22}, {'entity': 'LABEL_0', 'score': 0.62011474, 'index': 7, 'word': '##TL', 'start': 22, 'end': 24}, {'entity': 'LABEL_0', 'score': 0.57298166, 'index': 8, 'word': 'for', 'start': 25, 'end': 28}, {'entity': 'LABEL_0', 'score': 0.64287645, 'index': 9, 'word': 'car', 'start': 29, 'end': 32}, {'entity': 'LABEL_0', 'score': 0.6728945, 'index': 10, 'word': '##cas', 'start': 32, 'end': 35}, {'entity': 'LABEL_0', 'score': 0.6390939, 'index': 11, 'word': '##s', 'start': 35, 'end': 36}, {'entity': 'LABEL_0', 'score': 0.551114, 'index': 12, 'word': 'composition', 'start': 37, 'end': 48}, {'entity': 'LABEL_0', 'score': 0.5024227, 'index': 13, 'word': 'and', 'start': 49, 'end': 52}, {'entity': 'LABEL_0', 'score': 0.6793031, 'index': 14, 'word': 'meat', 'start': 53, 'end': 57}, {'entity': 'LABEL_0', 'score': 0.58751756, 'index': 15, 'word': 'quality', 'start': 58, 'end': 65}, {'entity': 'LABEL_0', 'score': 0.6234206, 'index': 16, 'word': 'were', 'start': 66, 'end': 70}, {'entity': 'LABEL_0', 'score': 0.5465201, 'index': 17, 'word': 'identified', 'start': 71, 'end': 81}, {'entity': 'LABEL_1', 'score': 0.5134602, 'index': 18, 'word': 'in', 'start': 82, 'end': 84}, {'entity': 'LABEL_1', 'score': 0.5254017, 'index': 19, 'word': 'a', 'start': 85, 'end': 86}, {'entity': 'LABEL_1', 'score': 0.51433605, 'index': 20, 'word': 'commercial', 'start': 87, 'end': 97}, {'entity': 'LABEL_1', 'score': 0.5712894, 'index': 21, 'word': 'finish', 'start': 98, 'end': 104}, {'entity': 'LABEL_1', 'score': 0.5325293, 'index': 22, 'word': '##er', 'start': 104, 'end': 106}, {'entity': 'LABEL_1', 'score': 0.53854746, 'index': 23, 'word': 'cross', 'start': 107, 'end': 112}, {'entity': 'LABEL_0', 'score': 0.5626948, 'index': 24, 'word': '.', 'start': 112, 'end': 113}, {'entity': 'LABEL_0', 'score': 0.62672335, 'index': 25, 'word': 'The', 'start': 114, 'end': 117}, {'entity': 'LABEL_0', 'score': 0.6544035, 'index': 26, 'word': 'main', 'start': 118, 'end': 122}, {'entity': 'LABEL_0', 'score': 0.52059317, 'index': 27, 'word': 'objective', 'start': 123, 'end': 132}, {'entity': 'LABEL_0', 'score': 0.61560094, 'index': 28, 'word': 'of', 'start': 133, 'end': 135}, {'entity': 'LABEL_0', 'score': 0.59127945, 'index': 29, 'word': 'the', 'start': 136, 'end': 139}, {'entity': 'LABEL_0', 'score': 0.53075016, 'index': 30, 'word': 'current', 'start': 140, 'end': 147}, {'entity': 'LABEL_0', 'score': 0.6014959, 'index': 31, 'word': 'study', 'start': 148, 'end': 153}, {'entity': 'LABEL_0', 'score': 0.56112796, 'index': 32, 'word': 'was', 'start': 154, 'end': 157}, {'entity': 'LABEL_0', 'score': 0.57662886, 'index': 33, 'word': 'to', 'start': 158, 'end': 160}, {'entity': 'LABEL_1', 'score': 0.56239766, 'index': 34, 'word': 'confirm', 'start': 161, 'end': 168}, {'entity': 'LABEL_1', 'score': 0.544427, 'index': 35, 'word': 'and', 'start': 169, 'end': 172}, {'entity': 'LABEL_0', 'score': 0.5102443, 'index': 36, 'word': 'fine', 'start': 173, 'end': 177}, {'entity': 'LABEL_0', 'score': 0.5760249, 'index': 37, 'word': 'map', 'start': 178, 'end': 181}, {'entity': 'LABEL_0', 'score': 0.5500853, 'index': 38, 'word': 'the', 'start': 182, 'end': 185}, {'entity': 'LABEL_0', 'score': 0.5414059, 'index': 39, 'word': 'Q', 'start': 186, 'end': 187}, {'entity': 'LABEL_0', 'score': 0.59261435, 'index': 40, 'word': '##TL', 'start': 187, 'end': 189}, {'entity': 'LABEL_1', 'score': 0.52226424, 'index': 41, 'word': 'on', 'start': 190, 'end': 192}, {'entity': 'LABEL_0', 'score': 0.60816616, 'index': 42, 'word': 'SS', 'start': 193, 'end': 195}, {'entity': 'LABEL_1', 'score': 0.5213098, 'index': 43, 'word': '##C', 'start': 195, 'end': 196}, {'entity': 'LABEL_0', 'score': 0.50582564, 'index': 44, 'word': '##4', 'start': 196, 'end': 197}, {'entity': 'LABEL_1', 'score': 0.5129648, 'index': 45, 'word': 'and', 'start': 198, 'end': 201}, {'entity': 'LABEL_0', 'score': 0.61623657, 'index': 46, 'word': 'SS', 'start': 202, 'end': 204}, {'entity': 'LABEL_1', 'score': 0.53512144, 'index': 47, 'word': '##C', 'start': 204, 'end': 205}, {'entity': 'LABEL_1', 'score': 0.5227248, 'index': 48, 'word': '##11', 'start': 205, 'end': 207}, {'entity': 'LABEL_0', 'score': 0.548529, 'index': 49, 'word': 'by', 'start': 208, 'end': 210}, {'entity': 'LABEL_0', 'score': 0.57738996, 'index': 50, 'word': 'g', 'start': 211, 'end': 212}, {'entity': 'LABEL_0', 'score': 0.7133712, 'index': 51, 'word': '##eno', 'start': 212, 'end': 215}, {'entity': 'LABEL_0', 'score': 0.7083621, 'index': 52, 'word': '##ty', 'start': 215, 'end': 217}, {'entity': 'LABEL_0', 'score': 0.64787793, 'index': 53, 'word': '##ping', 'start': 217, 'end': 221}, {'entity': 'LABEL_0', 'score': 0.502369, 'index': 54, 'word': 'an', 'start': 222, 'end': 224}, {'entity': 'LABEL_1', 'score': 0.55687195, 'index': 55, 'word': 'increased', 'start': 225, 'end': 234}, {'entity': 'LABEL_0', 'score': 0.5665773, 'index': 56, 'word': 'number', 'start': 235, 'end': 241}, {'entity': 'LABEL_0', 'score': 0.5718085, 'index': 57, 'word': 'of', 'start': 243, 'end': 245}, {'entity': 'LABEL_1', 'score': 0.5017501, 'index': 58, 'word': 'individuals', 'start': 246, 'end': 257}, {'entity': 'LABEL_0', 'score': 0.5312478, 'index': 59, 'word': 'and', 'start': 258, 'end': 261}, {'entity': 'LABEL_1', 'score': 0.50103915, 'index': 60, 'word': 'markers', 'start': 262, 'end': 269}, {'entity': 'LABEL_0', 'score': 0.5198209, 'index': 61, 'word': 'and', 'start': 270, 'end': 273}, {'entity': 'LABEL_0', 'score': 0.58493274, 'index': 62, 'word': 'to', 'start': 274, 'end': 276}, {'entity': 'LABEL_0', 'score': 0.5698395, 'index': 63, 'word': 'analyze', 'start': 277, 'end': 284}, {'entity': 'LABEL_0', 'score': 0.5910424, 'index': 64, 'word': 'the', 'start': 285, 'end': 288}, {'entity': 'LABEL_0', 'score': 0.53875214, 'index': 65, 'word': 'data', 'start': 289, 'end': 293}, {'entity': 'LABEL_0', 'score': 0.59574246, 'index': 66, 'word': 'using', 'start': 294, 'end': 299}, {'entity': 'LABEL_0', 'score': 0.55806816, 'index': 67, 'word': 'a', 'start': 300, 'end': 301}, {'entity': 'LABEL_0', 'score': 0.51098883, 'index': 68, 'word': 'combined', 'start': 302, 'end': 310}, {'entity': 'LABEL_0', 'score': 0.5680177, 'index': 69, 'word': 'link', 'start': 311, 'end': 315}, {'entity': 'LABEL_0', 'score': 0.66903615, 'index': 70, 'word': '##age', 'start': 315, 'end': 318}, {'entity': 'LABEL_1', 'score': 0.534407, 'index': 71, 'word': 'and', 'start': 319, 'end': 322}, {'entity': 'LABEL_0', 'score': 0.5577061, 'index': 72, 'word': 'link', 'start': 323, 'end': 327}, {'entity': 'LABEL_0', 'score': 0.7027598, 'index': 73, 'word': '##age', 'start': 327, 'end': 330}, {'entity': 'LABEL_0', 'score': 0.5513065, 'index': 74, 'word': 'di', 'start': 331, 'end': 333}, {'entity': 'LABEL_0', 'score': 0.63248605, 'index': 75, 'word': '##se', 'start': 333, 'end': 335}, {'entity': 'LABEL_0', 'score': 0.61532086, 'index': 76, 'word': '##qui', 'start': 335, 'end': 338}, {'entity': 'LABEL_0', 'score': 0.571514, 'index': 77, 'word': '##li', 'start': 338, 'end': 340}, {'entity': 'LABEL_0', 'score': 0.6172339, 'index': 78, 'word': '##bri', 'start': 340, 'end': 343}, {'entity': 'LABEL_0', 'score': 0.58652383, 'index': 79, 'word': '##um', 'start': 343, 'end': 345}, {'entity': 'LABEL_0', 'score': 0.552938, 'index': 80, 'word': 'analysis', 'start': 346, 'end': 354}, {'entity': 'LABEL_0', 'score': 0.53071374, 'index': 81, 'word': 'method', 'start': 355, 'end': 361}, {'entity': 'LABEL_1', 'score': 0.5918781, 'index': 82, 'word': '.', 'start': 361, 'end': 362}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dmis-lab/biobert-v1.1\", tokenizer=\"dmis-lab/biobert-v1.1\")\n",
    "\n",
    "text = \"In a previous study, QTL for carcass composition and meat quality were identified in a commercial finisher cross. The main objective of the current study was to confirm and fine map the QTL on SSC4 and SSC11 by genotyping an increased number  of individuals and markers and to analyze the data using a combined linkage and linkage disequilibrium analysis method.\"\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "print(entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
