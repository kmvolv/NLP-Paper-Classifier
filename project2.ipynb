{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting QTL json file to CSV format\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def json_to_csv(json_file, csv_file):\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "    with open(csv_file, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(data[0].keys())\n",
    "        for row in data:\n",
    "            writer.writerow(row.values())\n",
    "    \n",
    "json_path = \"QTL_text.json\"\n",
    "csv_path = \"QTL_text.csv\"\n",
    "json_to_csv(json_path, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into test and train dataframes\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"QTL_text.csv\")\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.1, stratify=df[\"Category\"])\n",
    "\n",
    "train.to_csv(\"train.csv\", index=False)\n",
    "test.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow<2.11\n",
      "  Downloading tensorflow-2.10.1-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow<2.11) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow<2.11) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow<2.11) (25.2.10)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow<2.11)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow<2.11) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow<2.11) (3.13.0)\n",
      "Collecting keras-preprocessing>=1.1.1 (from tensorflow<2.11)\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow<2.11) (18.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow<2.11) (1.26.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow<2.11) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow<2.11) (24.2)\n",
      "Collecting protobuf<3.20,>=3.9.2 (from tensorflow<2.11)\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-win_amd64.whl.metadata (806 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow<2.11) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow<2.11) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow<2.11) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow<2.11) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow<2.11) (1.17.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow<2.11) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow<2.11) (1.70.0)\n",
      "Collecting tensorboard<2.11,>=2.10 (from tensorflow<2.11)\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0 (from tensorflow<2.11)\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.11,>=2.10.0 (from tensorflow<2.11)\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.11) (0.45.1)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.11,>=2.10->tensorflow<2.11)\n",
      "  Downloading google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.11,>=2.10->tensorflow<2.11)\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11) (2.28.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.11,>=2.10->tensorflow<2.11)\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.11,>=2.10->tensorflow<2.11)\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11) (3.1.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\rohai\\anaconda3\\envs\\nlp\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow<2.11) (2.1.5)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading tensorflow-2.10.1-cp310-cp310-win_amd64.whl (455.9 MB)\n",
      "   ---------------------------------------- 0.0/455.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.5/455.9 MB 26.9 MB/s eta 0:00:17\n",
      "    --------------------------------------- 8.4/455.9 MB 22.6 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 12.6/455.9 MB 24.6 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 18.4/455.9 MB 23.2 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 25.7/455.9 MB 25.1 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 31.2/455.9 MB 25.7 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 37.0/455.9 MB 25.8 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 39.8/455.9 MB 24.4 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 45.9/455.9 MB 24.7 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 51.9/455.9 MB 25.2 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 57.7/455.9 MB 25.7 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 63.4/455.9 MB 25.6 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 68.2/455.9 MB 25.6 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 74.4/455.9 MB 25.5 MB/s eta 0:00:15\n",
      "   ------ --------------------------------- 78.6/455.9 MB 25.3 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 84.9/455.9 MB 25.6 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 90.2/455.9 MB 25.6 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 96.5/455.9 MB 25.8 MB/s eta 0:00:14\n",
      "   -------- ------------------------------ 101.7/455.9 MB 25.8 MB/s eta 0:00:14\n",
      "   --------- ----------------------------- 107.0/455.9 MB 25.8 MB/s eta 0:00:14\n",
      "   --------- ----------------------------- 113.2/455.9 MB 25.9 MB/s eta 0:00:14\n",
      "   ---------- ---------------------------- 118.5/455.9 MB 25.7 MB/s eta 0:00:14\n",
      "   ---------- ---------------------------- 123.2/455.9 MB 25.6 MB/s eta 0:00:13\n",
      "   ----------- --------------------------- 129.0/455.9 MB 25.8 MB/s eta 0:00:13\n",
      "   ----------- --------------------------- 134.7/455.9 MB 25.8 MB/s eta 0:00:13\n",
      "   ------------ -------------------------- 140.8/455.9 MB 25.9 MB/s eta 0:00:13\n",
      "   ------------ -------------------------- 146.8/455.9 MB 26.1 MB/s eta 0:00:12\n",
      "   ------------- ------------------------- 152.8/455.9 MB 26.0 MB/s eta 0:00:12\n",
      "   ------------- ------------------------- 158.3/455.9 MB 26.1 MB/s eta 0:00:12\n",
      "   ------------- ------------------------- 162.5/455.9 MB 25.9 MB/s eta 0:00:12\n",
      "   -------------- ------------------------ 168.6/455.9 MB 26.0 MB/s eta 0:00:12\n",
      "   -------------- ------------------------ 172.0/455.9 MB 26.0 MB/s eta 0:00:11\n",
      "   --------------- ----------------------- 176.2/455.9 MB 25.6 MB/s eta 0:00:11\n",
      "   --------------- ----------------------- 177.7/455.9 MB 25.0 MB/s eta 0:00:12\n",
      "   --------------- ----------------------- 183.8/455.9 MB 25.1 MB/s eta 0:00:11\n",
      "   ---------------- ---------------------- 189.8/455.9 MB 25.2 MB/s eta 0:00:11\n",
      "   ---------------- ---------------------- 194.0/455.9 MB 25.3 MB/s eta 0:00:11\n",
      "   ---------------- ---------------------- 198.2/455.9 MB 25.0 MB/s eta 0:00:11\n",
      "   ----------------- --------------------- 203.4/455.9 MB 25.0 MB/s eta 0:00:11\n",
      "   ----------------- --------------------- 208.9/455.9 MB 24.9 MB/s eta 0:00:10\n",
      "   ------------------ -------------------- 215.0/455.9 MB 25.1 MB/s eta 0:00:10\n",
      "   ------------------ -------------------- 219.4/455.9 MB 25.0 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 222.6/455.9 MB 24.7 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 228.9/455.9 MB 24.8 MB/s eta 0:00:10\n",
      "   -------------------- ------------------ 234.9/455.9 MB 24.9 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 239.3/455.9 MB 24.8 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 246.2/455.9 MB 25.0 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 252.7/455.9 MB 25.1 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 258.7/455.9 MB 25.2 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 265.0/455.9 MB 25.2 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 269.0/455.9 MB 25.2 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 273.9/455.9 MB 25.1 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 280.2/455.9 MB 25.3 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 285.2/455.9 MB 25.3 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 289.4/455.9 MB 25.0 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 292.6/455.9 MB 24.8 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 298.1/455.9 MB 24.7 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 303.3/455.9 MB 24.9 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 308.8/455.9 MB 24.9 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 313.5/455.9 MB 24.9 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 319.8/455.9 MB 24.8 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 325.3/455.9 MB 24.8 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 330.8/455.9 MB 24.9 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 335.8/455.9 MB 24.8 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 340.5/455.9 MB 24.8 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 346.3/455.9 MB 24.8 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 352.3/455.9 MB 24.8 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 357.3/455.9 MB 24.7 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 362.8/455.9 MB 24.8 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 368.6/455.9 MB 24.7 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 374.3/455.9 MB 24.8 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 375.4/455.9 MB 24.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 381.4/455.9 MB 24.5 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 386.7/455.9 MB 24.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 392.2/455.9 MB 24.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 398.5/455.9 MB 24.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 404.2/455.9 MB 24.5 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 410.3/455.9 MB 24.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 416.5/455.9 MB 24.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 421.5/455.9 MB 24.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 427.8/455.9 MB 24.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 433.1/455.9 MB 24.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 436.2/455.9 MB 24.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 440.7/455.9 MB 24.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  446.7/455.9 MB 24.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  452.2/455.9 MB 24.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  455.9/455.9 MB 24.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  455.9/455.9 MB 24.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  455.9/455.9 MB 24.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 455.9/455.9 MB 23.7 MB/s eta 0:00:00\n",
      "Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 22.9 MB/s eta 0:00:00\n",
      "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Downloading protobuf-3.19.6-cp310-cp310-win_amd64.whl (895 kB)\n",
      "   ---------------------------------------- 0.0/895.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 895.7/895.7 kB 39.6 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "   ---------------------------------------- 0.0/5.9 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 5.2/5.9 MB 24.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.9/5.9 MB 24.0 MB/s eta 0:00:00\n",
      "Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "Downloading google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "   ---------------------------------------- 0.0/781.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 781.3/781.3 kB 34.6 MB/s eta 0:00:00\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: tensorboard-plugin-wit, keras, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, oauthlib, keras-preprocessing, gast, cachetools, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.8.0\n",
      "    Uninstalling keras-3.8.0:\n",
      "      Successfully uninstalled keras-3.8.0\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.2\n",
      "    Uninstalling tensorboard-data-server-0.7.2:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.3\n",
      "    Uninstalling protobuf-5.29.3:\n",
      "      Successfully uninstalled protobuf-5.29.3\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.6.0\n",
      "    Uninstalling gast-0.6.0:\n",
      "      Successfully uninstalled gast-0.6.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.18.0\n",
      "    Uninstalling tensorboard-2.18.0:\n",
      "      Successfully uninstalled tensorboard-2.18.0\n",
      "Successfully installed cachetools-5.5.2 gast-0.4.0 google-auth-2.38.0 google-auth-oauthlib-0.4.6 keras-2.10.0 keras-preprocessing-1.1.2 oauthlib-3.2.2 protobuf-3.19.6 pyasn1-0.6.1 pyasn1-modules-0.4.1 requests-oauthlib-2.0.0 rsa-4.9 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.1 tensorflow-estimator-2.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.18.0 requires keras>=3.5.0, but you have keras 2.10.0 which is incompatible.\n",
      "tensorflow-intel 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-intel 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.10.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install \"tensorflow<2.11\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas, numpy, string\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, decomposition, ensemble, naive_bayes\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# import xgboost\n",
    "# from keras import layers, models, optimizers\n",
    "# from keras.preprocessing import text, sequence\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test_unlabeled.tsv and convert to csv\n",
    "# test_df = pd.read_csv(\"test_unlabeled.tsv\", sep='\\t')\n",
    "# test_df.to_csv(\"test_unlabeled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_val = pd.read_csv(\"test.csv\")\n",
    "df_test = pd.read_csv(\"test_unlabeled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "0    9244\n",
       "1     906\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Helper Functions\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import re, unidecode, string\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "def remove_numbers(text): \n",
    "    result = re.sub(r'\\d+', '', text) \n",
    "    return result\n",
    "def remove_slash_with_space(text): \n",
    "    return text.replace('\\\\', \" \")\n",
    "def remove_punctuation(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator) \n",
    "def text_lowercase(text): \n",
    "    return text.lower()     \n",
    "def remove_whitespace(text): \n",
    "    return  \" \".join(text.split()) \n",
    "def remove_stopwords(text): \n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
    "    return ' '.join(filtered_text)\n",
    "def stem_words(text): \n",
    "    stemmer = PorterStemmer() \n",
    "    word_tokens = word_tokenize(text) \n",
    "    stems = [stemmer.stem(word) for word in word_tokens] \n",
    "    return ' '.join(stems)\n",
    "def lemmatize_words(text): \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    word_tokens = word_tokenize(text) \n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens] \n",
    "    return ' '.join(lemmas) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform preprocessing\n",
    "def perform_preprocessing(text):\n",
    "    text = remove_accented_chars(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = text_lowercase(text)\n",
    "    text = remove_slash_with_space(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = stem_words(text)\n",
    "    text = lemmatize_words(text)\n",
    "    text = remove_whitespace(text)\n",
    "    return text\n",
    "\n",
    "df_train['Title'] = df_train['Title'].apply(perform_preprocessing)\n",
    "df_train['Abstract'] = df_train['Abstract'].apply(perform_preprocessing)\n",
    "df_val['Title'] = df_val['Title'].apply(perform_preprocessing)\n",
    "df_val['Abstract'] = df_val['Abstract'].apply(perform_preprocessing)\n",
    "df_test['Title'] = df_test['Title'].apply(perform_preprocessing)\n",
    "df_test['Abstract'] = df_test['Abstract'].apply(perform_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train['Abstract']\n",
    "X_val = df_val['Abstract']\n",
    "y_train = df_train['Category']\n",
    "y_val = df_val['Category']\n",
    "X_test = df_test['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectors\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(X_train)\n",
    "X_train_count =  count_vect.transform(X_train)\n",
    "X_val_count =  count_vect.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf =  tfidf_vect.transform(X_train)\n",
    "X_val_tfidf =  tfidf_vect.transform(X_val)\n",
    "X_test_tfidf =  tfidf_vect.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,2), max_features=5000)\n",
    "tfidf_vect_ngram.fit(X_train)\n",
    "X_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "X_val_tfidf_ngram =  tfidf_vect_ngram.transform(X_val)\n",
    "X_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in each class in training set:\n",
      "Category\n",
      "0    9244\n",
      "1     906\n",
      "Name: count, dtype: int64\n",
      "Category\n",
      "0    9244\n",
      "1    9244\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Oversampling using Random Over Sampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=3)\n",
    "\n",
    "X_train_tfidf_ngram_rand, y_train_rand = ros.fit_resample(X_train_tfidf_ngram, y_train)\n",
    "\n",
    "print(\"Number of samples in each class in training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(y_train_rand.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in each class in training set:\n",
      "Category\n",
      "0    9244\n",
      "1     906\n",
      "Name: count, dtype: int64\n",
      "Category\n",
      "0    9244\n",
      "1    9244\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Oversampling using SMOTE\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_tfidf_ngram_smoted, y_train_smoted = sm.fit_resample(X_train_tfidf_ngram, y_train)\n",
    "\n",
    "# Print the number of samples in each class\n",
    "print(\"Number of samples in each class in training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(y_train_smoted.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Accuracy:  0.9618794326241135\n",
      "{0: 1046, 1: 82}\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression on Ngram Level TF IDF Vectors\n",
    "LRClassifier = linear_model.LogisticRegression()\n",
    "LRClassifier.fit(X_train_tfidf_ngram, y_train)\n",
    "predictions = LRClassifier.predict(X_val_tfidf_ngram)\n",
    "acc = metrics.accuracy_score(predictions, y_val)\n",
    "print(\"LR, Accuracy: \", acc)\n",
    "\n",
    "# show me the unique values in prediction and their counts\n",
    "unique, counts = numpy.unique(predictions, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "test_predictions = LRClassifier.predict(X_test_tfidf_ngram)\n",
    "\n",
    "# Create a new dataframe with the PMID and its corresponding prediction\n",
    "df_test['Category'] = test_predictions\n",
    "df_test = df_test[['PMID', 'Category']]\n",
    "df_test.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [8:23:53<00:00, 302.33s/trial, best loss: -0.9684106080262396]  \n",
      "best:\n",
      "{'colsample_bytree': 0.49, 'gamma': 0.47000000000000003, 'learning_rate': 0.08, 'max_depth': 11, 'min_child_weight': 1.0, 'n_estimators': 4, 'scale_pos_weight': 1.0, 'subsample': 0.7000000000000001}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for XGBoost classifier with hyperopt\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def hyperopt_train_test(params):\n",
    "    xgbClassifier = xgboost.XGBClassifier(**params)\n",
    "    return cross_val_score(xgbClassifier, X_train_tfidf_ngram_smoted.tocsc(), y_train_smoted, scoring='f1_weighted').mean()\n",
    "\n",
    "space4xgb = {\n",
    "    'learning_rate': hp.quniform('learning_rate', 0.01, 0.3, 0.01),\n",
    "    'max_depth': hp.choice('max_depth', range(5, 30, 1)),\n",
    "    'n_estimators': hp.choice('n_estimators', range(20, 205, 5)),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'subsample': hp.quniform('subsample', 0.1, 1, 0.01),\n",
    "    'gamma': hp.quniform('gamma', 0, 0.5, 0.01),\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.1, 1.0, 0.01),\n",
    "    'scale_pos_weight': hp.quniform('scale_pos_weight', 1, 10, 1)\n",
    "}\n",
    "\n",
    "def f(params):\n",
    "    acc = hyperopt_train_test(params)\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(f, space4xgb, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "print('best:')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB, F1 Score:  0.9203383569739952\n",
      "[[966  61]\n",
      " [ 23  78]]\n",
      "{0: 989, 1: 139}\n"
     ]
    }
   ],
   "source": [
    "# XGBoost on Ngram Level TF IDF Vectors\n",
    "\n",
    "xgbClassifier = xgboost.XGBClassifier(colsample_bytree = 0.49, gamma = 0.47000000000000003, learning_rate=0.08, max_depth = 11, min_child_weight = 1, n_estimators=4, scale_pos_weight=1, subsample=0.7000000000000001)\n",
    "xgbClassifier.fit(X_train_tfidf_ngram_smoted.tocsc(), y_train_smoted)\n",
    "\n",
    "predictions = xgbClassifier.predict(X_val_tfidf_ngram.tocsc())\n",
    "f1 = metrics.f1_score(predictions, y_val, average='weighted')\n",
    "print(\"XGB, F1 Score: \", f1)\n",
    "\n",
    "#print confusion matrix\n",
    "print(metrics.confusion_matrix(y_val, predictions))\n",
    "\n",
    "unique, counts = numpy.unique(predictions, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "test_predictions = xgbClassifier.predict(X_test_tfidf_ngram.tocsc())\n",
    "\n",
    "# Create a new dataframe with the PMID and its corresponding prediction\n",
    "df_test['Category'] = test_predictions\n",
    "df_test = df_test[['PMID', 'Category']]\n",
    "df_test.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Accuracy:  0.8874113475177305\n",
      "{0: 910, 1: 218}\n",
      "[[905 122]\n",
      " [  5  96]]\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes on Ngram Level TF IDF Vectors\n",
    "NBClassifier = naive_bayes.MultinomialNB(alpha=0.1)\n",
    "NBClassifier.fit(X_train_tfidf_ngram_smoted, y_train_smoted)\n",
    "predictions = NBClassifier.predict(X_val_tfidf_ngram)\n",
    "acc = metrics.accuracy_score(predictions, y_val)\n",
    "print(\"NB, Accuracy: \", acc)\n",
    "\n",
    "unique, counts = numpy.unique(predictions, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "print(metrics.confusion_matrix(y_val, predictions))\n",
    "\n",
    "test_predictions = NBClassifier.predict(X_test_tfidf_ngram.tocsc())\n",
    "\n",
    "# Create a new dataframe with the PMID and its corresponding prediction\n",
    "df_test['Category'] = test_predictions\n",
    "df_test = df_test[['PMID', 'Category']]\n",
    "df_test.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'C': 1, 'class_weight': {0: 1, 1: 3}}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning for SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10], 'class_weight': [{0: 1, 1: 3}, {0: 1, 1: 5}, 'balanced']}\n",
    "grid_search = GridSearchCV(svm.SVC(kernel='linear', probability=True), param_grid, scoring='recall')\n",
    "grid_search.fit(X_train_tfidf_ngram_smoted, y_train_smoted)\n",
    "\n",
    "print(\"Best Params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Accuracy:  0.9547872340425532\n",
      "{0: 992, 1: 136}\n",
      "[[984  43]\n",
      " [  8  93]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Category\n",
       "0    1006\n",
       "1      91\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Classifier on Ngram Level TF IDF Vectors\n",
    "SVMClassifier = svm.SVC(C=1, kernel='linear', class_weight='balanced', probability=True)\n",
    "SVMClassifier.fit(X_train_tfidf_ngram, y_train)\n",
    "predictions = SVMClassifier.predict(X_val_tfidf_ngram)\n",
    "\n",
    "acc = metrics.accuracy_score(predictions, y_val)\n",
    "print(\"SVM, Accuracy: \", acc)\n",
    "\n",
    "unique, counts = numpy.unique(predictions, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "print(metrics.confusion_matrix(y_val, predictions))\n",
    "\n",
    "\n",
    "test_predictions = SVMClassifier.predict(X_test_tfidf_ngram)\n",
    "\n",
    "\n",
    "# Create a new dataframe with the PMID and its corresponding prediction\n",
    "df_test['Category'] = test_predictions\n",
    "\n",
    "df_test['Category'].value_counts()\n",
    "# df_test = df_test[['PMID', 'Category']]\n",
    "# df_test.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with threshold\n",
    "probs = SVMClassifier.predict_proba(X_test_tfidf_ngram)\n",
    "threshold = 0.4\n",
    "predictions = (probs[:,1] >= threshold).astype('int')\n",
    "\n",
    "# test_predictions = SVMClassifier.predict(X_test_tfidf_ngram)\n",
    "\n",
    "\n",
    "# Create a new dataframe with the PMID and its corresponding prediction\n",
    "df_test['Category'] = predictions\n",
    "df_test = df_test[['PMID', 'Category']]\n",
    "df_test.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "0.9489405483199876\n",
      "KNeighborsClassifier(metric='manhattan', n_neighbors=3, weights='distance')\n",
      "{'mean_fit_time': array([0.01670041, 0.018787  , 0.01700034, 0.01810703, 0.01791053,\n",
      "       0.02119756, 0.0191978 , 0.01990247, 0.01770821, 0.01820345,\n",
      "       0.01890483, 0.01910219, 0.01799836, 0.019804  , 0.01920633,\n",
      "       0.02031393]), 'std_fit_time': array([0.00231443, 0.00250998, 0.00309869, 0.00476054, 0.00430797,\n",
      "       0.00665245, 0.0074157 , 0.0026519 , 0.00159385, 0.00133496,\n",
      "       0.00294249, 0.00290431, 0.00189761, 0.00376387, 0.00417476,\n",
      "       0.00130383]), 'mean_score_time': array([ 3.94989834,  3.92046518,  4.33785043,  4.31243343,  5.0637589 ,\n",
      "        4.30111208,  4.18091264,  3.96510425, 12.91361585, 11.77229381,\n",
      "       12.41022248, 11.68885608, 12.61834717, 11.454528  , 13.83981276,\n",
      "       12.13556418]), 'std_score_time': array([0.16560931, 0.20581771, 0.13217201, 0.30105246, 0.459033  ,\n",
      "       0.18274686, 0.09017453, 0.10078784, 0.1375323 , 0.18318008,\n",
      "       0.17441313, 0.32921791, 0.17474706, 0.11805872, 1.41455959,\n",
      "       0.3984453 ]), 'param_metric': masked_array(data=['euclidean', 'euclidean', 'euclidean', 'euclidean',\n",
      "                   'euclidean', 'euclidean', 'euclidean', 'euclidean',\n",
      "                   'manhattan', 'manhattan', 'manhattan', 'manhattan',\n",
      "                   'manhattan', 'manhattan', 'manhattan', 'manhattan'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_neighbors': masked_array(data=[3, 3, 5, 5, 7, 7, 9, 9, 3, 3, 5, 5, 7, 7, 9, 9],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value=999999), 'param_weights': masked_array(data=['uniform', 'distance', 'uniform', 'distance',\n",
      "                   'uniform', 'distance', 'uniform', 'distance',\n",
      "                   'uniform', 'distance', 'uniform', 'distance',\n",
      "                   'uniform', 'distance', 'uniform', 'distance'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'uniform'}, {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}, {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'uniform'}, {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'distance'}, {'metric': 'euclidean', 'n_neighbors': 7, 'weights': 'uniform'}, {'metric': 'euclidean', 'n_neighbors': 7, 'weights': 'distance'}, {'metric': 'euclidean', 'n_neighbors': 9, 'weights': 'uniform'}, {'metric': 'euclidean', 'n_neighbors': 9, 'weights': 'distance'}, {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}, {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'distance'}, {'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'uniform'}, {'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}, {'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'uniform'}, {'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'distance'}, {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'uniform'}, {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'distance'}], 'split0_test_score': array([0.78664143, 0.78691184, 0.74932396, 0.75013521, 0.72336398,\n",
      "       0.72444565, 0.70118983, 0.70281233, 0.90237966, 0.91725257,\n",
      "       0.85343429, 0.88994051, 0.80692266, 0.86154678, 0.76446728,\n",
      "       0.83288264]), 'split1_test_score': array([0.79367226, 0.79421309, 0.7566252 , 0.75716604, 0.73201731,\n",
      "       0.73255814, 0.71633315, 0.71795565, 0.94267171, 0.96241211,\n",
      "       0.90886966, 0.94780963, 0.86992969, 0.93347756, 0.83585722,\n",
      "       0.92049757]), 'split2_test_score': array([0.77366144, 0.77447269, 0.73715522, 0.73769605, 0.71146566,\n",
      "       0.71254732, 0.69442942, 0.69632234, 0.93510005, 0.95429962,\n",
      "       0.90265008, 0.94267171, 0.86533261, 0.92942131, 0.83342347,\n",
      "       0.91968632]), 'split3_test_score': array([0.79334596, 0.79415742, 0.75520692, 0.75601839, 0.72328915,\n",
      "       0.7246416 , 0.7013795 , 0.70273194, 0.92750879, 0.95428726,\n",
      "       0.89369759, 0.94265621, 0.8520422 , 0.92832026, 0.81065729,\n",
      "       0.90532864]), 'split4_test_score': array([0.78874763, 0.78928861, 0.74844468, 0.74898566, 0.72680552,\n",
      "       0.72761699, 0.70516635, 0.70597782, 0.93318907, 0.95645118,\n",
      "       0.89153368, 0.94373817, 0.8571815 , 0.92750879, 0.82472275,\n",
      "       0.91344333]), 'mean_test_score': array([0.78721374, 0.78780873, 0.7493512 , 0.75000027, 0.72338832,\n",
      "       0.72436194, 0.70369965, 0.70516002, 0.92816986, 0.94894055,\n",
      "       0.89003706, 0.93336324, 0.85028173, 0.91605494, 0.8138256 ,\n",
      "       0.8983677 ]), 'std_test_score': array([0.00728919, 0.00724108, 0.00688135, 0.00692831, 0.00675717,\n",
      "       0.00659459, 0.00720244, 0.00712613, 0.01377748, 0.01612035,\n",
      "       0.01933472, 0.02179393, 0.02255343, 0.02733121, 0.02620877,\n",
      "       0.03319107]), 'rank_test_score': array([10,  9, 12, 11, 14, 13, 16, 15,  3,  1,  6,  2,  7,  4,  8,  5])}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "KNNClassifier = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(KNNClassifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_tfidf_ngram_smoted, y_train_smoted)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN, Accuracy:  0.9432624113475178\n",
      "{0: 1075, 1: 53}\n"
     ]
    }
   ],
   "source": [
    "# KNN Classifier on Ngram Level TF IDF Vectors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "KNNClassifier = KNeighborsClassifier(n_neighbors=3, weights='distance', metric='manhattan')\n",
    "KNNClassifier.fit(X_train_tfidf_ngram_smoted, y_train_smoted)\n",
    "predictions = KNNClassifier.predict(X_val_tfidf_ngram)\n",
    "\n",
    "acc = metrics.accuracy_score(predictions, y_val)\n",
    "print(\"KNN, Accuracy: \", acc)\n",
    "\n",
    "unique, counts = numpy.unique(predictions, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "test_predictions = KNNClassifier.predict(X_test_tfidf_ngram)\n",
    "\n",
    "# Create a new dataframe with the PMID and its corresponding prediction\n",
    "df_test['Category'] = test_predictions\n",
    "df_test = df_test[['PMID', 'Category']]\n",
    "df_test.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1: 0.942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rohai\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EE, F1 Score:  0.946611557317012\n",
      "{0: 1039, 1: 89}\n",
      "[[1002   25]\n",
      " [  37   64]]\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Classifier on Ngram Level TF IDF Vectors\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = EasyEnsembleClassifier(n_estimators=10, sampling_strategy='not majority')\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X_train_tfidf_ngram, y_train, scoring='f1_weighted', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Mean F1: %.3f' % numpy.mean(n_scores))\n",
    "\n",
    "model.fit(X_train_tfidf_ngram, y_train)\n",
    "predictions = model.predict(X_val_tfidf_ngram)\n",
    "f1 = metrics.f1_score(predictions, y_val, average='weighted')\n",
    "print(\"EE, F1 Score: \", f1)\n",
    "\n",
    "unique, counts = numpy.unique(predictions, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "print(metrics.confusion_matrix(y_val, predictions))\n",
    "\n",
    "test_predictions = model.predict(X_test_tfidf_ngram)\n",
    "\n",
    "# Create a new dataframe with the PMID and its corresponding prediction\n",
    "df_test['Category'] = test_predictions\n",
    "df_test = df_test[['PMID', 'Category']]\n",
    "df_test.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, F1 Score:  0.9540715811637186\n",
      "{0: 1065, 1: 63}\n",
      "[[1017   10]\n",
      " [  48   53]]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier on Ngram Level TF IDF Vectors\n",
    "RFClassifier = ensemble.RandomForestClassifier(n_estimators=1000, class_weight='balanced_subsample')\n",
    "RFClassifier.fit(X_train_tfidf_ngram, y_train)\n",
    "predictions = RFClassifier.predict(X_val_tfidf_ngram)\n",
    "\n",
    "f1 = metrics.f1_score(predictions, y_val, average='weighted')\n",
    "print(\"RF, F1 Score: \", f1)\n",
    "\n",
    "unique, counts = numpy.unique(predictions, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "print(metrics.confusion_matrix(y_val, predictions))\n",
    "\n",
    "test_predictions = RFClassifier.predict(X_test_tfidf_ngram)\n",
    "\n",
    "# Create a new dataframe with the PMID and its corresponding prediction\n",
    "df_test['Category'] = test_predictions\n",
    "df_test = df_test[['PMID', 'Category']]\n",
    "df_test.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1046, 1: 51}\n"
     ]
    }
   ],
   "source": [
    "preds = pd.read_csv(\"predictions.csv\")\n",
    "\n",
    "unique, counts = numpy.unique(preds['Category'], return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
